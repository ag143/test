# Pods
A pod is a collection of containers sharing a network, acting as the basic unit of deployment in Kubernetes. All containers in a pod are scheduled on the same node.

To launch a pod using the container imagequay.io/openshiftlabs/simpleservice:0.5.0 and exposing a HTTP API on port 9876, execute:

> kubectl run sise --image=quay.io/openshiftlabs/simpleservice:0.5.0 --port=9876

Warning: Older releases of kubectl will produce a deployment resource as the result of the provided kubectl run example, while newer releases produce a single pod resource. The example commands in this section should still work (assuming you substitute your own pod name) - but you'll need to run kubectl delete deployment sise at the end of this section to clean up.
Check to see if the pod is running:
> kubectl get pods

Resulting in output similar to the following:

```
NAME READY STATUS RESTARTS AGE
sise 1/1 Running 0 1m
```
If the above output returns a longer pod name, make sure to use it in the following examples (in place of sise).

This container image happens to include a copy of curl, which provides an additional way to verify that the primary webservice process is responding (over the local net at least):

> kubectl exec sise -t -- curl -s localhost:9876/info

This call should produce the output:
```
{"host": "localhost:9876", "version": "0.5.0", "from": "127.0.0.1"}
```
From within the cluster (e.g. via kubectl exec) this pod will also be directly accessible via it's associated pod IP 172.17.0.3:

> kubectl describe pod sise | grep IP:

The kubernetes proxy API provides an additional opportunity to make external connections to pods within the cluster using `curl`:

```
export K8S_API="https://$(kubectl config get-clusters | tail -n 1)"
export API_TOKEN="$(kubectl config view -o jsonpath={.users[-1].user.token})"
export NAMESPACE="default"
export PODNAME="sise"
curl -s -k -H"Authorization: Bearer $API_TOKEN" $K8S_API/api/v1/namespaces/$NAMESPACE/pods/$PODNAME/proxy/info
```
Cleanup:

> kubectl delete pod,deployment sise

Creating from a Manifest
 

You can also create a pod from a manifest file. In this case the pod is running the already known simpleservice image from above along with a generic CentOS container:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pods/pod.yaml

You can verify the created pod using the same command as above:

> kubectl get pods

The output reflects that there is a single pod, however it has two containers in it:
```
NAME                        READY   STATUS    RESTARTS   AGE
twocontainers               2/2     Running   0          36s
```
Containers that share a pod are able to communicate using local networking.

This example demonstrates how to exec into a sidecar shell container to access and inspect the sise container via localhost:

> kubectl exec twocontainers -t -- curl -s localhost:9876/info

Define the resources attribute to influence how much CPU and/or RAM a container in a pod can use (this example uses 64MB of RAM and 0.5 CPUs):

> kubectl create -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pods/constraint-pod.yaml

The `describe` subcommand displays more information about a specific pod than `get` does.

> kubectl describe pod constraintpod

There is quite a bit of information provided, with the resource limits displayed in the following snippets:

```
---
Containers:
sise:
---
Limits:
cpu: 500m
memory: 64Mi
Requests:
cpu: 500m
memory: 64Mi
---
```
Learn more about resource constraints in Kubernetes via the docs here and here.

To clean up and remove all the remaining pods, run:
> kubectl delete pod twocontainers
> kubectl delete pod constraintpod
To sum up, launching one or more containers (together) in Kubernetes is simple, however doing it directly as shown above comes with a serious limitation: you have to manually take care of keeping them running in case of a failure. A better way to supervise pods is to use deployments, giving you much more control over the life cycle, including rolling out a new version.
# Lables
Labels are the mechanism used to organize Kubernetes objects. A label is a key-value pair with certain restrictions concerning length and allowed values but without any pre-defined meaning. You're free to choose labels as you see fit, for example, to express environments such as "this pod is running in production" or ownership, like "department X owns that pod".

Let's create a pod that initially has one label (env=development):

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/labels/pod.yaml

The get subcommand can be used to display a pod's labels:

> kubectl get pods --show-labels

The labels are rendered as an additional column in the output:
```
NAME      READY   STATUS    RESTARTS   AGE   LABELS
labelex   1/1     Running   0          6s    env=development
```
You can add a label to the pod through the label subcommand:

> kubectl label pods labelex owner=michael

Running the get subcommand from above shows the new label in addition to the existing one:
```
NAME      READY   STATUS    RESTARTS   AGE   LABELS
labelex   1/1     Running   0          65s   env=development,owner=michael
```
To use a label for filtering, for example to list only pods that have an "owner" that equals "michael", use the --selector option:
> kubectl get pods --selector owner=michael

The --selector option can be abbreviated to -l, so selecting pods that are labelled with env=development can also be done using:

> kubectl get pods -l env=development

Oftentimes, Kubernetes objects support set-based selectors. Let's launch another pod that has two labels (env=production and owner=michael):

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/labels/anotherpod.yaml

Now, let's list all pods that are either labelled with env=development or with env=production:

> kubectl get pods -l 'env in (production, development)'

Since we have each pod has one of those two labels, they both appear in the output:
```
NAME           READY   STATUS    RESTARTS   AGE
labelex        1/1     Running   0          6m39s
labelexother   1/1     Running   0          46s
```
Other verbs also support label selection. For example, you could remove both of these pods with the same selector:

> kubectl delete pods -l 'env in (production, development)'

Beware that this will destroy any pods with those labels.  

You can also delete them directly, via their names, with:
> kubectl delete pods labelex

> kubectl delete pods labelexother
Note that labels are not restricted to pods. In fact you can apply them to all sorts of objects, such as nodes or services.
# Deployments
A deployment is a supervisor for pods, giving you fine-grained control over how and when a new pod version is rolled out as well as rolled back to a previous state.  

Let's create a deployment called sise-deploy that produces two replicas of a pod as well as a replica set:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d09.yaml

You can have a look at the deployment, as well as the the replica set and the pods the deployment looks using the get subcommand (multiple resource types may be specified in a single call):

> kubectl get pod,replicaset,deployment

The result is separated by resource type and reflects all of the resources created by the deployment:
```
NAME                               READY   STATUS    RESTARTS   AGE
pod/sise-deploy-747848cd97-d2hkf   1/1     Running   0          73s
pod/sise-deploy-747848cd97-klr7z   1/1     Running   0          73s

NAME                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/sise-deploy-747848cd97   2         2         2       74s

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/sise-deploy   2/2     2            2           74s
```
Note the naming of the pods and replica set, derived from the deployment name.
 

At this point in time the "sise" containers running in the pods are configured to return the version 0.9. Let's verify this from within the cluster using curl:
> kubectl exec sise-deploy-747848cd97-d2hkf -t -- curl -s 127.0.0.1:9876/info

The output reflects the current version of the deployed application:

> {"host": "127.0.0.1:9876", "version": "0.9", "from": "127.0.0.1"}
Let's now see what happens if we change that version to 1.0 in an updated deployment:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/deployments/d10.yaml

Note that you could have used kubectl edit deploy/sise-deploy instead to achieve the same by manually editing the deployment.  

Running the kubectl get pods command shows the rollout of two new pods with the updated version 1.0 as well as the two old pods with version 0.9 being terminated:
```
NAME                           READY   STATUS        RESTARTS   AGE
sise-deploy-67fd84bd5c-g2kkw   1/1     Running       0          35s
sise-deploy-67fd84bd5c-s6bkv   1/1     Running       0          32s
sise-deploy-747848cd97-d2hkf   1/1     Terminating   0          6m47s
sise-deploy-747848cd97-klr7z   0/1     Terminating   0          6m47s
```
Additionally, a new replica set is created as well:
```
NAME                     DESIRED   CURRENT   READY   AGE
sise-deploy-67fd84bd5c   2         2         2       51s
sise-deploy-747848cd97   0         0         0       7m3s
```
Note that during the deployment you can check the progress using kubectl rollout status deploy/sise-deploy.

To verify that if the new 1.0 version is really available, we execute from within the cluster (again using kubectl get pods get the name of one of the pods):

> kubectl exec sise-deploy-67fd84bd5c-g2kkw -t -- curl -s 127.0.0.1:9876/info

The resulting output reflects the new version of the container image:

> {"host": "127.0.0.1:9876", "version": "1.0", "from": "127.0.0.1"}
A history of all deployments is available via the rollout history subcommand:

> kubectl rollout history deploy/sise-deploy

If there are problems in the deployment Kubernetes will automatically roll back to the previous version, however you can also explicitly roll back to a specific revision, as in our case to revision 1 (the original pod version):

> kubectl rollout undo deploy/sise-deploy --to-revision=1

At this point in time we're back at where we started, with two new pods again serving version 0.9, which can be verified with the curl command from above:

> {"host": "127.0.0.1:9876", "version": "0.9", "from": "127.0.0.1"}
Finally, to clean up, we remove the deployment. Kubernetes will delete any child resources (in this case, the replica sets and pods):
> kubectl delete deploy sise-deploy

See also the documentation for more options on deployments and when they are triggered.
# Services
A service is an abstraction for pods, providing a stable, so called virtual IP (VIP) address. While pods may come and go and with it their IP addresses, a service allows clients to reliably connect to the containers running in the pod using the VIP. The "virtual" in VIP means it is not an actual IP address connected to a network interface, but its purpose is purely to forward traffic to one or more pods. Keeping the mapping between the VIP and the pods up-to-date is the job of kube-proxy, a process that runs on every node, which queries the API server to learn about new services in the cluster.

Let's create a new pod supervised by a replication controller and a service along with it:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/services/rc.yaml
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/services/svc.yaml
Verify the pod is running:
> kubectl get pods -l app=sise

A new pod name should be generated each time this example is run. Make sure to include your own pod name when running the following examples:
> kubectl describe pod rcsise-nlfzs

The output should appear similar to the following (which has been truncated for readability):
```
Name:         rcsise-nlfzs
Namespace:    default
Priority:     0
Node:         minikube/192.168.39.51
Start Time:   Thu, 03 Jun 2021 16:52:41 -0400
Labels:       app=sise
Annotations:  <none>
Status:       Running
IP:           172.17.0.3
IPs:
  IP:           172.17.0.3
Controlled By:  ReplicationController/rcsise
Containers:
  sise:
    Container ID:   docker://907686f441b6f30d8723c1a66ce9348955cded4934573d9a6703173f38a2d705
    Image:          quay.io/openshiftlabs/simpleservice:0.5.0
```
You can, from within the cluster, access the pod directly via its assigned IP 172.17.0.3:
> kubectl exec rcsise-nlfzs -t -- curl -s 172.17.0.3:9876/info

This is however, as mentioned above, not advisable since the IPs assigned to pods may change as pods are migrated or rescheduled. The service created at the start of this lesson, simpleservice, is used to abstract the access to the pod away from a specific IP:
> kubectl get service

The service resource type uses labels to identify which pods it will forward traffic to. In our case, pods labeled with app=sise will receive traffic.
 
From within the cluster, we can now access any affiliated pods using the IP address of the simpleservice service endpoint on port 80. KubeDNS even provides basic name resolution for Kubernetes services (within the same Kubernetes namespace). This allows us to connect to pods using the associated service name - no need to including IP addresses or port numbers.
> kubectl exec rcsise-6nq3k -t -- curl -s simpleservice/info

Let’s now add a second pod by scaling up the RC supervising it:

> kubectl scale --replicas=2 rc/rcsise

Wait for both pods to report they are in the "Running" state:

> kubectl get pods -l app=sise

This causes the traffic to the service being equally split between our two pods.
 
You can remove all of the resources created by running:
> kubectl delete svc simpleservice

> kubectl delete rc rcsise

# PortForwarding
In the context of developing applications on Kubernetes, it is often useful to quickly access a service from your local environment without exposing it using, for example, a load balancer or an ingress resource. In these situations, you can use port forwarding.

Let's create an application consisting of a deployment and a service named simpleservice, serving on port 80:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pf/app.yaml

Let's say we want to access the simpleservice service from the local environment on port 8080. Traffic can be forwarded to your local system using the port-forward subcommand:

> kubectl port-forward service/simpleservice 8080:80

This command does not immediately complete; as long as it is running, the port forward will be in use. We can verify the service is accessible by accessing it over port 8080 (you'll need a separate terminal):

Now we can invoke the service locally like so (using a separate terminal session):

curl localhost:8080/info
The output should resemble the following:

> {"host": "localhost:8080", "version": "0.5.0", "from": "127.0.0.1"}
Remember that port forwarding is not meant for production traffic, but for development and experimentation.
 
Disconnect the port forward by pressing CTRL+C in the terminal it's running in. The application can be cleaned up using the following:
> kubectl delete service/simpleservice deployment/sise-deploy

# HealthChecks
In order to verify if a container in a pod is healthy and ready to serve traffic, Kubernetes provides for a range of health checking mechanisms. Health checks, or probes as they are called in Kubernetes, are carried out by the kubelet to determine when to restart a container (liveness probes) and used by services and deployments to determine if a pod should receive traffic (readiness probes). We will focus on HTTP health checks in the following. Note that it is the responsibility of the application developer to expose a URL that the kubelet can use to determine if the container is healthy (and potentially ready).

Let's create a pod that exposes an endpoint /health, responding with a HTTP 200 status code:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/healthz/pod.yaml

In the pod specification we've defined the following:

```
livenessProbe:
initialDelaySeconds: 2
periodSeconds: 5
httpGet:
path: /health
port: 9876
```
The configuration above tells Kubernetes to start checking the /health endpoint, after initially waiting 2 seconds, every 5 seconds.  

If we now look at the pod we can see that it is considered healthy:
> kubectl describe pod hc

The following (truncated) output shows the relevant sections:
```
Name:         hc
Namespace:    default
Priority:     0
Node:         minikube/192.168.39.51

Containers:
  sise:
    Container ID:   docker://2cfe4187808a89ae4731abfe242ac42611e1f658505691f540ac31ca8f6ce86f
    Image:          quay.io/openshiftlabs/simpleservice:0.5.0
    ---
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:9876/health delay=2s timeout=1s period=5s #success=1 #failure=3
    Environment:    <none>
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
```
Now launch a bad pod which will randomly (in the time range 1 to 4 sec) not return a 200 code:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/healthz/badpod.yaml

Looking at the events of the bad pod, we can see that the health check failed:

> kubectl describe pod badpod

In particular, look at the events section at the bottom:
Events:
```
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  24s               default-scheduler  Successfully assigned default/badpod to minikube
  Normal   Pulled     22s               kubelet            Container image "quay.io/openshiftlabs/simpleservice:0.5.0" already present on machine
  Normal   Created    22s               kubelet            Created container sise
  Normal   Started    22s               kubelet            Started container sise
  Warning  Unhealthy  9s (x3 over 19s)  kubelet            Liveness probe failed: Get "http://172.17.0.4:9876/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Normal   Killing    9s                kubelet            Container sise failed liveness probe, will be restarted
  ```
This can also be verified with the get subcommand:

> kubectl get pods

Notice that badpod has been restarted multiple times because of the failing health checks.
```
NAME     READY   STATUS    RESTARTS   AGE
badpod   1/1     Running   2          109s
hc       1/1     Running   0          11m
```
In addition to a liveness probe, you can also specify a readiness probe. Readiness probes are configured in the same way, but have different use cases and semantics. The readiness probe indicates when the application itself is running and able to receive traffic.

Let's create a pod with a readiness probe that reports success after 10 seconds:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/healthz/ready.yaml

Looking at the events of the pod, we can see that, eventually, the pod is ready to serve traffic:

> kubectl describe pod ready

Depending on how quickly you ran the describe command, you may have noticed the pod reflected that it was not ready to receive traffic:
```
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
```
You can remove all of the created pods with:
> kubectl delete pod/hc pod/ready pod/badpod

Learn more about configuring probes, including TCP and command probes, in the documentation.
# EnvironemntVariables
You can set environment variables for containers running in a pod. Additionally, Kubernetes automatically exposes certain runtime information via environment variables.

Let's launch a pod that we pass an environment variable SIMPLE_SERVICE_VERSION with the value 1.0:
 

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/envs/pod.yaml
 

Now, let's verify from within the cluster if the application running in the pod has picked up the environment variable:
 

> kubectl exec envs -t -- curl -s 127.0.0.1:9876/info

The output reflects the value that was set for the environment variable (the default, unless overridden by the variable, is 0.5.0):

> {"host": "127.0.0.1:9876", "version": "1.0", "from": "127.0.0.1"}


You can check what environment variables Kubernetes itself provides automatically using a REST endpoint in the sample application:

 
 
> kubectl exec envs -t -- curl -s 127.0.0.1:9876/env

Your results will vary slightly depending on you cluster configuration, but an example output is included below:
```
{"version": "1.0", "env": "{'HOSTNAME': 'envs', 'DOCKER_REGISTRY_SERVICE_PORT': '5000', 'KUBERNETES_PORT_443_TCP_ADDR': '172.30.0.1', 'ROUTER_PORT_80_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_53_UDP_PROTO': 'udp', 'ROUTER_SERVICE_HOST': '172.30.246.127', 'ROUTER_PORT_1936_TCP_PROTO': 'tcp', 'KUBERNETES_SERVICE_PORT_DNS': '53', 'DOCKER_REGISTRY_PORT_5000_TCP_PORT': '5000', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'ROUTER_SERVICE_PORT_443_TCP': '443', 'KUBERNETES_PORT_53_TCP': 'tcp://172.30.0.1:53', 'KUBERNETES_SERVICE_PORT': '443', 'ROUTER_PORT_80_TCP_ADDR': '172.30.246.127', 'LANG': 'C.UTF-8', 'KUBERNETES_PORT_53_TCP_ADDR': '172.30.0.1', 'PYTHON_VERSION': '2.7.13', 'KUBERNETES_SERVICE_HOST': '172.30.0.1', 'PYTHON_PIP_VERSION': '9.0.1', 'DOCKER_REGISTRY_PORT_5000_TCP_PROTO': 'tcp', 'REFRESHED_AT': '2017-04-24T13:50', 'ROUTER_PORT_1936_TCP': 'tcp://172.30.246.127:1936', 'KUBERNETES_PORT_53_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_53_TCP_PORT': '53', 'HOME': '/root', 'DOCKER_REGISTRY_SERVICE_HOST': '172.30.1.1', 'GPG_KEY': 'C01E1CAD5EA2C4F0B8E3571504C367C218ADD4FF', 'ROUTER_SERVICE_PORT_80_TCP': '80', 'ROUTER_PORT_443_TCP_ADDR': '172.30.246.127', 'ROUTER_PORT_1936_TCP_ADDR': '172.30.246.127', 'ROUTER_SERVICE_PORT': '80', 'ROUTER_PORT_443_TCP_PORT': '443', 'KUBERNETES_SERVICE_PORT_DNS_TCP': '53', 'KUBERNETES_PORT_53_UDP_ADDR': '172.30.0.1', 'KUBERNETES_PORT_53_UDP': 'udp://172.30.0.1:53', 'KUBERNETES_PORT': 'tcp://172.30.0.1:443', 'ROUTER_PORT_1936_TCP_PORT': '1936', 'ROUTER_PORT_80_TCP': 'tcp://172.30.246.127:80', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'KUBERNETES_PORT_53_UDP_PORT': '53', 'ROUTER_PORT_80_TCP_PORT': '80', 'ROUTER_PORT': 'tcp://172.30.246.127:80', 'ROUTER_PORT_443_TCP': 'tcp://172.30.246.127:443', 'SIMPLE_SERVICE_VERSION': '1.0', 'ROUTER_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP': 'tcp://172.30.0.1:443', 'DOCKER_REGISTRY_PORT_5000_TCP': 'tcp://172.30.1.1:5000', 'DOCKER_REGISTRY_PORT': 'tcp://172.30.1.1:5000', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'ROUTER_SERVICE_PORT_1936_TCP': '1936', 'DOCKER_REGISTRY_PORT_5000_TCP_ADDR': '172.30.1.1', 'DOCKER_REGISTRY_SERVICE_PORT_5000_TCP': '5000', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp'}"}
```


 
Alternatively, you can also use the exec subcommand to display the environment variables within the pod:
> kubectl exec envs -- printenv
 

Remove the sample pod with:
 

> kubectl delete pod/envs
 

In addition to the above examples, you can also use secrets, volumes, or the downward API to inject additional information into your container environments.
# Namespaces
Namespaces provide a scope for Kubernetes resources, carving up your cluster in smaller units. You can think of it as a workspace you're sharing with other users. Many resources such as pods and services are namespaced. Others, such as nodes, are not namespaced, but are instead treated as cluster-wide. As a developer, you'll usually use an assigned namespace, however admins may wish to manage them, for example to set up access control or resource quotas.

Like other resources, the get subcommand displays a list of all namespaces a user has access to in a cluster (both the full resource type name namespace and the abbreviation ns can be used):

> kubectl get ns

On a simple minikube installation, the result shows:
```
NAME              STATUS   AGE
default           Active   17h
kube-node-lease   Active   17h
kube-public       Active   17h
kube-system       Active   17h
```
You can learn more about a namespace using the describe verb:

> kubectl describe ns default

If no changes were made to the minikube cluster, the output should look like the following:
```
Name:         default
Labels:       <none>
Annotations:  <none>
Status:       Active
```
No resource quota.

No LimitRange resource.
Let's now create a new namespace called "test":
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/ns/ns.yaml

Once the namespace is created, it will appear in the list of available namespaces:

> kubectl get ns

Alternatively, we could have created the namespace using the > kubectl create namespace test command.
 
To launch a pod in the newly created namespace test, run:
kubectl apply --namespace=test -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/ns/pod.yaml
Note that using above method the namespace becomes a runtime property. In other words, you can deploy the same pod or service into multiple namespaces (for example, dev and prod). Hard-coding the namespace directly in the metadata section as shown in the following is possible, but causes less flexibility when deploying your apps:
```
apiVersion: v1
kind: Pod
metadata:
name: podintest
namespace: test
```
To list namespaced objects, such as our pod podintest, pass the --namespace variable to the get call:

> kubectl get pods --namespace=test

You can remove the namespace (and everything inside of it) with:

> kubectl delete ns test

If you're an admin, you might want to check out the docs for more info how to handle namespaces.
# Volumes
A Kubernetes volume is essentially a directory accessible to all containers running in a pod. In contrast to the container-local filesystem, the data in volumes is preserved across container restarts. The medium backing a volume and its contents are determined by the volume type:  
node-local types such as emptyDir or hostPath
file-sharing types such as nfs
cloud provider-specific types like awsElasticBlockStore, azureDisk, or gcePersistentDisk
distributed file system types, for example glusterfs or cephfs
special-purpose types like secret, gitRepo
A special type of volume is PersistentVolume, which is covered in its own lesson.
 
Let's create a pod with two containers that use an emptyDir volume to exchange data:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/volumes/pod.yaml

Volume information is displayed in the detailed output:
> kubectl describe pod sharevol

The output below is truncated to show the relevant volume information:
```
Name:         sharevol
Namespace:    default
Priority:     0
Node:         minikube/192.168.39.51

Containers:
  c1:
    Container ID:  docker://0cfe351e5a3131d3e02ca92a4aad8ea196cde403dcbc4713329bb418e1cce144
    ---
    Mounts:
      /tmp/xchange from xchange (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sckql (ro)
  c2:
    Container ID:  docker://93eadd487c18f5fc77885b8c343dff6891c2fdbae9752160a7d5a08c2763ba9c
    ---
    Mounts:
      /tmp/data from xchange (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sckql (ro)
---
Volumes:
  xchange:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
```
We first connect to one of the containers in the pod, c1, to view the volume mount and generate some data:

> kubectl exec -it sharevol -c c1 -- bash

The volume is mounted like any other Linux volume mount:

mount | grep xchange
Create a file in the mount that we'll be able to access from the other container in the pod:

echo 'some data' > /tmp/xchange/data
When you're finished, disconnect from the container:

exit
When we now connect to the container c2, the second container running in the pod, we can see the volume mounted at /tmp/data (as compared to c1 where it is mounted to /tmp/xchange) and are able to read the data created in the previous step:

> kubectl exec -it sharevol -c c2 -- bash

cat /tmp/data/data
Once again, exit from the connected container by running exit.  

Note that in each container you need to decide where to mount the volume, and that for emptyDir you currently can not specify resource consumption limits.
 
You can remove the pod with:
> kubectl delete pod/sharevol

As already described, this will destroy the shared volume and all its contents.
# PersistentVolumes
A persistent volume (PV) is a cluster-wide resource that you can use to store data in a way that it persists beyond the lifetime of a pod. The PV is not backed by locally-attached storage on a worker node but by networked storage system such as EBS or NFS or a distributed filesystem like Ceph.

Depending on your cluster and storage type, the configuration of a PV will vary slightly. The command below will work for minikube clusters, where it will create the volume as a mount on the minikube VM:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pv/pv.yaml

In order to use a PV, you need to claim it first, using a persistent volume claim (PVC). The PVC requests a PV with your desired specification (size, speed, etc.) from Kubernetes and binds it to a pod where you it is mounted as a volume. Let's create a PVC, asking Kubernetes for 1 GB of storage using the default storage class:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pv/pvc.yaml

Persistent Volume Claims can be queried using the abbreviation pvc:

> kubectl get pvc

The output reflects the name of the created PVC and some basic information:
```
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myclaim   Bound    pv0001   5Gi        RWO            standard       10s
```
To understand how the persistence works, let's create a deployment that uses above PVC and mounts it as a volume into /tmp/persistent:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/pv/deploy.yaml

Now we want to test if data in the volume actually persists. Start by finding the pod managed by above deployment using kubectl get pods (it will begin with pv-deploy-) and then connecting into it:

> kubectl exec $POD_NAME -it -- /bin/bash

Once in the pod, create a new file in the mounted persistent volume:

echo "Hello World" > /tmp/persistent/test
Once you've verified the file was created, disconnect from the pod using exit.

It's time to destroy the pod and let the deployment launch a new pod. The expectation is that the PV will be reconnected to the new pod and the data in /tmp/persistent is still present:
> kubectl delete pod $POD_NAME

Since Kubernetes will ensure that the desired number of pods in a deployment are present, it will start a new pod with the same spec as the original. When viewing the list of pods, you will likely see two: the previous pod (which should be in the "Terminating" state) and the new pod being started. For example (your pod names will vary slightly):
```
NAME                         READY   STATUS        RESTARTS   AGE
pv-deploy-7d5f79cb7f-d4jb6   1/1     Terminating   0          4m32s
pv-deploy-7d5f79cb7f-fnscf   1/1     Running       0          19s
```
Connect to the newly created pod (the one in the "Running" state) as you did previously:

> kubectl exec $POD_NAME -it -- /bin/bash

Display the contents of the file that was created from the original pod:

cat /tmp/persistent/test 
The text Hello World should appear, showing that the original persistent volume was mounted to the newly created pod.

Note that the default behavior is that even when the deployment is deleted, the PVC and the PV continue to exist. This storage protection feature helps avoid data loss. Once you're sure you don't need the data anymore, you can go ahead and delete the PVC (for the purposes of this lesson, we'll delete the PV as well). To clean up before the next lesson, we'll also delete the deployment:
> kubectl delete deployment pv-deploy
> kubectl delete pvc myclaim
> kubectl delete pv pv0001
The types of persistent volume available to your Kubernetes cluster depend on the environment (on-prem or public cloud). The Stateful Kubernetes site has more information on the types of volumes available.
# Secrets
You don't want sensitive information such as a database password or an API key stored in clear text. Secrets provide you with a mechanism to store such information in a safe and reliable way with the following properties:

Secrets are namespaced objects, that is, exist in the context of a specific namespace
You can access them via a volume or an environment variable from a container running in a pod
The secret data on nodes is stored in tmpfs volumes
A per-secret size limit of 1MB exists
The API server stores secrets as plaintext in etcd
Let's create a secret named apikey that holds an example API key. The first step is to create a file that contains the secret data:
> echo -n "A19fh68B001j" > ./apikey.txt
That file is passed to the command that creates the secret:

> kubectl create secret generic apikey --from-file=./apikey.txt

Information about the secret is retrieved using the describe subcommand:

> kubectl describe secrets/apikey

The value of the secret isn't displayed by default, but other metadata is shown:
```
Name: apikey
Namespace: default
Labels: <none>
Annotations: <none>

Type: Opaque

Data
====
apikey.txt: 12 bytes
```
Now let's use the secret in a pod through a volume:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/secrets/pod.yaml

Connect to the container to verify the attached secret:

> kubectl exec -it consumesec -c shell -- bash

The secret is mounted at /tmp/apikey:

> mount | grep apikey
The value of the key is stored in a file with the same name as the original file the secret was created from:

```
cat /tmp/apikey/apikey.txt
Disconnect from the running container by running exit.
``` 
 
Note that for service accounts, Kubernetes automatically creates secrets containing credentials for accessing the API and modifies your pods to use this type of secret.
 
You can remove both the pod and the secret with:
> kubectl delete pod/consumesec secret/apikey

# Logging
Logging is one option to understand what is going on inside your applications and the cluster at large. Basic logging in Kubernetes makes the output a container produces available through the kubectl tool. More advanced setups consider logs across nodes and store them in a central place, either within the cluster or via a dedicated (cloud-based) service.

Let's create a pod called logme that runs a container writing to stdout and stderr:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/logging/pod.yaml

To view the five most recent log lines of the gen container in the logme pod, execute:

> kubectl logs --tail=5 logme -c gen

Streaming functionality, similar to running tail -f, is available as well:

> kubectl logs -f --since=10s logme -c gen

Note that if you didn't specify --since=10s in the above command, you would have gotten all of the log lines from the start of the container.  

You can also view logs of pods that have already completed their lifecycle. To demonstrate this, create a pod called oneshot that counts down from 9 to 1 and then exits:
> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/logging/oneshotpod.yaml

By using the -p option, you can print the logs for previous instances of the container in a pod:

> kubectl logs -p oneshot -c gen

You can remove the created pods with:

> kubectl delete pod/logme pod/oneshot

# JObs
A job in Kubernetes is a supervisor for pods that run for a certain time to completion, for example a calculation or a backup operation.

Let's create a job named "countdown" that supervises a pod counting from 9 down to 1:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/jobs/job.yaml

The job definition is listed under the resource type job:

> kubectl get jobs

A job is executed as a pod. Unlike most pods, however, the pod spawned by a job does not continue to run, but will instead reach a "Completed" state. Below is an example output of the kubectl get pods command after a job has run.
```
NAME              READY   STATUS      RESTARTS   AGE
countdown-dzrz8   0/1     Completed   0          55s
```
Further details of the job can be seen in the describe subcommand:

```
Name:           countdown
Namespace:      default
Selector:       controller-uid=e5024398-6795-4583-8e74-431f57f54a3d
Labels:         controller-uid=e5024398-6795-4583-8e74-431f57f54a3d
                job-name=countdown
Annotations:    <none>
Parallelism:    1
Completions:    1
Start Time:     Sat, 05 Jun 2021 15:21:34 -0400
Completed At:   Sat, 05 Jun 2021 15:21:39 -0400
Duration:       5s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=e5024398-6795-4583-8e74-431f57f54a3d
           job-name=countdown
  Containers:
   counter:
    Image:      centos:7
    Port:       <none>
    Host Port:  <none>
    Command:
      bin/bash
      -c
      for i in 9 8 7 6 5 4 3 2 1 ; do echo $i ; done
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  2m34s  job-controller  Created pod: countdown-dzrz8
  Normal  Completed         2m30s  job-controller  Job completed
```
Since the job ran as a pod, the logs subcommand will show any output during its execution (the name of the pod is included in the events list as seen above):

> kubectl logs $POD_NAME

To clean up, use the delete verb on the job object which will remove all the supervised pods:

> kubectl delete job countdown

Note that there are also more advanced ways to use jobs, for example, by utilizing a work queue or scheduling the execution at a certain time through cron jobs.
# Nodes
In Kubernetes, nodes are the (potentially virtual) machines where your workloads run. As a developer, you typically don't deal with nodes directly, however as an admin
you might want to familiarize yourself with node operations.
 
Node information is captured in a resource type named node:
> kubectl get nodes

The output will vary depending on your cluster. The example below is taken from a minikube cluster:
```
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   42m   v1.20.2
```
One uncommon, but still important, requirement is to make Kubernetes schedule a pod on a certain node. For this, we first need to label the node we want to target (using the node name as retrieved above):
> kubectl label nodes minikube shouldrun=here

Now we can create a pod that is scheduled on the node with the label shouldrun=here:

> kubectl apply -f https://raw.githubusercontent.com/openshift-evangelists/kbe/main/specs/nodes/pod.yaml

The -o wide flag, when retrieving pod information, will show the node on which the pod is running:
> kubectl get pods --output=wide

In this case, the node is the same one that was labeled in the label command above.  

The describe subcommand contains a wealth of information about the node (the example output below has been truncated for readability):
```
Name:               minikube
Roles:              control-plane,master

Addresses:
  InternalIP:  192.168.39.147
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             11999700Ki
  pods:               110

Events:
  Type    Reason                   Age                From        Message
  ----    ------                   ----               ----        -------
  Normal  NodeHasSufficientMemory  47m (x7 over 47m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    47m (x6 over 47m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     47m (x6 over 47m)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 47m                kubelet     Starting kubelet.
```
Note that there are more sophisticated methods than shown above, such as using affinity, to assign pods to nodes.
# references
https://kubebyexample.com/concept/pods
